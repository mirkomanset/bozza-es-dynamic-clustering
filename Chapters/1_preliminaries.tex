\section{Preliminaries}\label{sec:preliminaries}

This section introduces foundational concepts essential for understanding
clustering in streaming environments and the challenges of evolving data
distributions.

\subsection{Clustering}\label{subsec:clustering}

Clustering partitions a dataset $D = \{\mathbf{x}_1, \dots, \mathbf{x}_N\}$
into $K$ clusters by optimizing an objective $\mathcal{J}(C; D)$ that
encourages high similarity within clusters and low similarity across clusters.
Classical methods include K-means, which assigns points to nearest centroids,
and Gaussian Mixture Models (GMMs), which model data as mixtures of Gaussian
distributions with parameters $\{\pi_j, \boldsymbol{\mu}_j,
    \mathbf{\Sigma}_j\}$ and provide soft assignments through posterior
probabilities.

\subsection{Streaming Learning}\label{subsec:streaming_learning}

In streaming environments, data arrives continuously as an unbounded sequence.
Streaming algorithms incrementally update a model $\mathcal{M}_t$ with each new
point $\mathbf{x}_t$:
\[
    \mathcal{M}_{t+1} = \mathcal{F}(\mathcal{M}_t, \mathbf{x}_t),
\]
balancing stability and adaptability under bounded memory and computational
constraints.

\subsection{Data Drift}\label{subsec:data_drift}

\textbf{Data drift}~\cite{drift_adaptation_survey} refers to changes in statistical properties over time, occurring when $p_t(X, y) \neq p_\tau(X, y)$ between times $t$ and $\tau$. \emph{Real drift} involves changes in the conditional distribution $p(y \mid X)$, while \emph{virtual drift} affects the marginal distribution $p(X)$ with stable $p(y \mid X)$.

Detection strategies monitor model performance, compare
distributions, or combine multiple signals. Adaptation mechanisms include
\emph{blind adaptation} (continuous updates) and \emph{informed adaptation}
(updates triggered by detected drift), employing retraining, incremental
updates, or ensemble strategies to maintain predictive accuracy under evolving
conditions.

\subsection{Drift Explainability}\label{subsec:drift_explainability}

Drift explainability identifies the causes and features responsible for
distributional changes, clarifying why model performance degrades over time.
\emph{Feature importance analysis} compares contributions $\phi_i$ between
reference and production models, with large discrepancies $\Delta \phi_i =
    |\phi_i^{\text{ref}} - \phi_i^{\text{prod}}|$ revealing influential
drift-driving variables. The \emph{Classifier Two-Sample Test} (C2ST) treats
reference and production datasets as separate classes, training a classifier to
distinguish them, feature importances from this classifier highlight dimensions
contributing most to distributional change.

Explainable detection methods ensure model updates are necessary and
effectively targeted, enhancing trustworthiness and reliability of adaptive
systems in dynamic environments.